{
    "metadata": {
        "kernelspec": {
            "name": "pyspark3kernel",
            "display_name": "PySpark3"
        },
        "language_info": {
            "name": "pyspark3",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "python",
                "version": 3
            },
            "pygments_lexer": "python3"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "<img src=\"https://github.com/Microsoft/sqlworkshops/blob/master/graphics/solutions-microsoft-logo-small.png?raw=true\" alt=\"Microsoft\">\r\n",
                "<br>\r\n",
                "\r\n",
                "# SQL Server 2019 big data cluster Tutorial\r\n",
                "## 05 - Using Spark For Machine Learning\r\n",
                "\r\n",
                "In this tutorial you will learn how to work with Spark Jobs in a SQL Server big data cluster. \r\n",
                "\r\n",
                "Wide World Importers has refridgerated trucks to deliver temperature-sensitive products. These are high-profit, and high-expense items. In the past, there have been failures in the cooling systems, and the primary culprit has been the deep-cycle batteries used in the system.\r\n",
                "\r\n",
                "WWI began replacing the batteriess every three months as a preventative measure, but this has a high cost. Recently, the taxes on recycling batteries has increased dramatically. The CEO has asked the Data Science team if they can investigate creating a Predictive Maintenance system to more accurately tell the maintenance staff how long a battery will last, rather than relying on a flat 3 month cycle. \r\n",
                "\r\n",
                "The trucks have sensors that transmit data to a file location. The trips are also logged. In this Jupyter Notebook, you'll create, train and store a Machine Learning model using SciKit-Learn, so that it can be deployed to multiple hosts. "
            ],
            "metadata": {
                "azdata_cell_guid": "969bbd54-5f8e-49eb-b466-5e05633fa7be"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import pickle \r\n",
                "import pandas as pd\r\n",
                "import numpy as np\r\n",
                "import datetime as dt\r\n",
                "from sklearn.linear_model import LogisticRegression\r\n",
                "from sklearn.model_selection import train_test_split"
            ],
            "metadata": {
                "azdata_cell_guid": "03e7dc98-c577-4616-9708-e82908019d40"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Starting Spark application\n",
                    "output_type": "stream"
                },
                {
                    "data": {
                        "text/plain": "<IPython.core.display.HTML object>",
                        "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1569595626385_0003</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"https://13.93.154.163:30443/gateway/default/yarn/proxy/application_1569595626385_0003/\">Link</a></td><td><a target=\"_blank\" href=\"https://13.93.154.163:30443/gateway/default/yarn/container/container_1569595626385_0003_01_000001/root\">Link</a></td><td>âœ”</td></tr></table>"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "text": "SparkSession available as 'spark'.\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 2
        },
        {
            "cell_type": "markdown",
            "source": [
                "First, download the sensor data from the location where it is transmitted from the trucks, and load it into a Spark DataFrame."
            ],
            "metadata": {
                "azdata_cell_guid": "18f250c3-d1fd-40e1-a652-10f948c8b0ab"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "df = pd.read_csv('https://cs7a9736a9346a1x44c6xb00.blob.core.windows.net/backups/training-formatted.csv', header=0)\r\n",
                "\r\n",
                "df.dropna()\r\n",
                "print(df.shape)\r\n",
                "print(list(df.columns))"
            ],
            "metadata": {
                "azdata_cell_guid": "da351ace-6907-411b-a1ab-e3e8a1be8111"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "(10000, 74)\n['Survival_In_Days', 'Province', 'Region', 'Trip_Length_Mean', 'Trip_Length_Sigma', 'Trips_Per_Day_Mean', 'Trips_Per_Day_Sigma', 'Battery_Rated_Cycles', 'Manufacture_Month', 'Manufacture_Year', 'Alternator_Efficiency', 'Car_Has_EcoStart', 'Twelve_hourly_temperature_history_for_last_31_days_before_death_last_recording_first', 'Sensor_Reading_1', 'Sensor_Reading_2', 'Sensor_Reading_3', 'Sensor_Reading_4', 'Sensor_Reading_5', 'Sensor_Reading_6', 'Sensor_Reading_7', 'Sensor_Reading_8', 'Sensor_Reading_9', 'Sensor_Reading_10', 'Sensor_Reading_11', 'Sensor_Reading_12', 'Sensor_Reading_13', 'Sensor_Reading_14', 'Sensor_Reading_15', 'Sensor_Reading_16', 'Sensor_Reading_17', 'Sensor_Reading_18', 'Sensor_Reading_19', 'Sensor_Reading_20', 'Sensor_Reading_21', 'Sensor_Reading_22', 'Sensor_Reading_23', 'Sensor_Reading_24', 'Sensor_Reading_25', 'Sensor_Reading_26', 'Sensor_Reading_27', 'Sensor_Reading_28', 'Sensor_Reading_29', 'Sensor_Reading_30', 'Sensor_Reading_31', 'Sensor_Reading_32', 'Sensor_Reading_33', 'Sensor_Reading_34', 'Sensor_Reading_35', 'Sensor_Reading_36', 'Sensor_Reading_37', 'Sensor_Reading_38', 'Sensor_Reading_39', 'Sensor_Reading_40', 'Sensor_Reading_41', 'Sensor_Reading_42', 'Sensor_Reading_43', 'Sensor_Reading_44', 'Sensor_Reading_45', 'Sensor_Reading_46', 'Sensor_Reading_47', 'Sensor_Reading_48', 'Sensor_Reading_49', 'Sensor_Reading_50', 'Sensor_Reading_51', 'Sensor_Reading_52', 'Sensor_Reading_53', 'Sensor_Reading_54', 'Sensor_Reading_55', 'Sensor_Reading_56', 'Sensor_Reading_57', 'Sensor_Reading_58', 'Sensor_Reading_59', 'Sensor_Reading_60', 'Sensor_Reading_61']",
                    "output_type": "stream"
                }
            ],
            "execution_count": 8
        },
        {
            "cell_type": "markdown",
            "source": [
                "After examining the data, the Data Science team selects certain columns that they believe are highly predictive of the battery life."
            ],
            "metadata": {
                "azdata_cell_guid": "12249759-7afa-402a-badc-9167ad70b5f1"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Select the features used for predicting battery life\r\n",
                "x = df.iloc[:,1:74]\r\n",
                "x = x.iloc[:,np.r_[2:7, 9:73]]\r\n",
                "x = x.interpolate() \r\n",
                "\r\n",
                "# Select the labels only (the measured battery life) \r\n",
                "y = df.iloc[:,0].values.flatten()\r\n",
                "print('Interpolation Complete')"
            ],
            "metadata": {
                "azdata_cell_guid": "0806c0d5-7f0d-4528-8ec4-57c21989717f"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Interpolation Complete",
                    "output_type": "stream"
                }
            ],
            "execution_count": 9
        },
        {
            "cell_type": "code",
            "source": [
                "# Examine the features selected \r\n",
                "print(list(x.columns))"
            ],
            "metadata": {
                "azdata_cell_guid": "05de0ddd-ece4-4005-82a6-02a2cd2946bb"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "['Trip_Length_Mean', 'Trip_Length_Sigma', 'Trips_Per_Day_Mean', 'Trips_Per_Day_Sigma', 'Battery_Rated_Cycles', 'Alternator_Efficiency', 'Car_Has_EcoStart', 'Twelve_hourly_temperature_history_for_last_31_days_before_death_last_recording_first', 'Sensor_Reading_1', 'Sensor_Reading_2', 'Sensor_Reading_3', 'Sensor_Reading_4', 'Sensor_Reading_5', 'Sensor_Reading_6', 'Sensor_Reading_7', 'Sensor_Reading_8', 'Sensor_Reading_9', 'Sensor_Reading_10', 'Sensor_Reading_11', 'Sensor_Reading_12', 'Sensor_Reading_13', 'Sensor_Reading_14', 'Sensor_Reading_15', 'Sensor_Reading_16', 'Sensor_Reading_17', 'Sensor_Reading_18', 'Sensor_Reading_19', 'Sensor_Reading_20', 'Sensor_Reading_21', 'Sensor_Reading_22', 'Sensor_Reading_23', 'Sensor_Reading_24', 'Sensor_Reading_25', 'Sensor_Reading_26', 'Sensor_Reading_27', 'Sensor_Reading_28', 'Sensor_Reading_29', 'Sensor_Reading_30', 'Sensor_Reading_31', 'Sensor_Reading_32', 'Sensor_Reading_33', 'Sensor_Reading_34', 'Sensor_Reading_35', 'Sensor_Reading_36', 'Sensor_Reading_37', 'Sensor_Reading_38', 'Sensor_Reading_39', 'Sensor_Reading_40', 'Sensor_Reading_41', 'Sensor_Reading_42', 'Sensor_Reading_43', 'Sensor_Reading_44', 'Sensor_Reading_45', 'Sensor_Reading_46', 'Sensor_Reading_47', 'Sensor_Reading_48', 'Sensor_Reading_49', 'Sensor_Reading_50', 'Sensor_Reading_51', 'Sensor_Reading_52', 'Sensor_Reading_53', 'Sensor_Reading_54', 'Sensor_Reading_55', 'Sensor_Reading_56', 'Sensor_Reading_57', 'Sensor_Reading_58', 'Sensor_Reading_59', 'Sensor_Reading_60', 'Sensor_Reading_61']",
                    "output_type": "stream"
                }
            ],
            "execution_count": 10
        },
        {
            "cell_type": "markdown",
            "source": [
                "The lead Data Scientist believes that a standard Regression algorithm would do the best predictions."
            ],
            "metadata": {
                "azdata_cell_guid": "88ff4ca6-d8ec-49ed-8e4e-915f1664527e"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Train a regression model \r\n",
                "from sklearn.ensemble import GradientBoostingRegressor \r\n",
                "model = GradientBoostingRegressor() \r\n",
                "model.fit(x,y)"
            ],
            "metadata": {
                "azdata_cell_guid": "0d8162d2-7530-44a7-8318-79ee11ffa210"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n             max_leaf_nodes=None, min_impurity_decrease=0.0,\n             min_impurity_split=None, min_samples_leaf=1,\n             min_samples_split=2, min_weight_fraction_leaf=0.0,\n             n_estimators=100, n_iter_no_change=None, presort='auto',\n             random_state=None, subsample=1.0, tol=0.0001,\n             validation_fraction=0.1, verbose=0, warm_start=False)",
                    "output_type": "stream"
                }
            ],
            "execution_count": 11
        },
        {
            "cell_type": "code",
            "source": [
                "# Try making a single prediction and observe the result \r\n",
                "model.predict(x.iloc[0:1]) "
            ],
            "metadata": {
                "azdata_cell_guid": "11818008-b450-4799-a779-a1b6de81093b"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "array([1323.39791998])",
                    "output_type": "stream"
                }
            ],
            "execution_count": 12
        },
        {
            "cell_type": "markdown",
            "source": [
                "After the model is trained, perform testing from labeled data."
            ],
            "metadata": {
                "azdata_cell_guid": "bdff0323-079d-4559-861a-345993135749"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# access the test data from HDFS by reading into a Spark DataFrame \r\n",
                "test_data = pd.read_csv('https://cs7a9736a9346a1x44c6xb00.blob.core.windows.net/backups/fleet-formatted.csv', header=0)\r\n",
                "test_data.dropna()\r\n",
                "\r\n",
                "# prepare the test data (dropping unused columns) \r\n",
                "test_data = test_data.drop(columns=[\"Car_ID\", \"Battery_Age\"])\r\n",
                "test_data = test_data.iloc[:,np.r_[2:7, 9:73]]\r\n",
                "test_data.rename(columns={'Twelve_hourly_temperature_forecast_for_next_31_days _reversed': 'Twelve_hourly_temperature_history_for_last_31_days_before_death_l ast_recording_first'}, inplace=True) \r\n",
                "# make the battery life predictions for each of the vehicles in the test data \r\n",
                "battery_life_predictions = model.predict(test_data) \r\n",
                "# examine the prediction \r\n",
                "battery_life_predictions"
            ],
            "metadata": {
                "azdata_cell_guid": "25465418-a277-47d8-b258-1e7f9540ffdb"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "array([1472.91111228, 1340.08897725, 1421.38601032, 1473.79033215,\n       1651.66584142, 1412.85617044, 1842.81351408, 1264.22762055,\n       1930.45602533, 1681.86345995])",
                    "output_type": "stream"
                }
            ],
            "execution_count": 13
        },
        {
            "cell_type": "code",
            "source": [
                "# prepare one data frame that includes predictions for each vehicle \r\n",
                "scored_data = test_data \r\n",
                "scored_data[\"Estimated_Battery_Life\"] = battery_life_predictions \r\n",
                "df_scored = spark.createDataFrame(scored_data) \r\n",
                "# Optionally, write out the scored data: \r\n",
                "# df_scored.coalesce(1).write.option(\"header\", \"true\").csv(\"/pdm\") "
            ],
            "metadata": {
                "azdata_cell_guid": "160ceaf9-81c1-4bb8-92ca-511a8b928cac"
            },
            "outputs": [],
            "execution_count": 16
        },
        {
            "cell_type": "markdown",
            "source": [
                "Once you are satisfied with the Model, you can save it out using the \"Pickle\" library for deployment to other systems."
            ],
            "metadata": {
                "azdata_cell_guid": "af10557a-dce4-42b4-8ba7-af1faaad461c"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "pickle_file = open('/tmp/pdm.pkl', 'wb')\r\n",
                "pickle.dump(model, pickle_file)\r\n",
                "import os\r\n",
                "print(os.getcwd())\r\n",
                "os.listdir('//tmp/')"
            ],
            "metadata": {
                "azdata_cell_guid": "75c70761-9c37-4446-85f4-71c4d32f6493"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "/tmp/nm-local-dir/usercache/root/appcache/application_1569595626385_0003/container_1569595626385_0003_01_000001\n['nm-local-dir', 'hsperfdata_root', 'hadoop-root-nodemanager.pid', 'hadoop-root-datanode.pid', 'jetty-0.0.0.0-8044-node-_-any-1399597149712545407.dir', 'jetty-localhost-43849-datanode-_-any-4367549175596043164.dir', 'pdm.pkl', 'tmpo7d6l6mt']",
                    "output_type": "stream"
                }
            ],
            "execution_count": 18
        },
        {
            "cell_type": "markdown",
            "source": [
                "**(Optional)**\r\n",
                "\r\n",
                "You could export this model and <a href=\"https://azure.microsoft.com/en-us/services/sql-database-edge/\" target=\"_blank\">run it at the edge or in SQL Server directly</a>. Here's an example of what that code could look like:\r\n",
                "\r\n",
                "<pre>\r\n",
                "\r\n",
                "DECLARE @query_string nvarchar(max) -- Query Truck Data\r\n",
                "SET @query_string='\r\n",
                "SELECT ['Trip_Length_Mean', 'Trip_Length_Sigma', 'Trips_Per_Day_Mean', 'Trips_Per_Day_Sigma', 'Battery_Rated_Cycles', 'Alternator_Efficiency', 'Car_Has_EcoStart', 'Twelve_hourly_temperature_history_for_last_31_days_before_death_last_recording_first', 'Sensor_Reading_1', 'Sensor_Reading_2', 'Sensor_Reading_3', 'Sensor_Reading_4', 'Sensor_Reading_5', 'Sensor_Reading_6', 'Sensor_Reading_7', 'Sensor_Reading_8', 'Sensor_Reading_9', 'Sensor_Reading_10', 'Sensor_Reading_11', 'Sensor_Reading_12', 'Sensor_Reading_13', 'Sensor_Reading_14', 'Sensor_Reading_15', 'Sensor_Reading_16', 'Sensor_Reading_17', 'Sensor_Reading_18', 'Sensor_Reading_19', 'Sensor_Reading_20', 'Sensor_Reading_21', 'Sensor_Reading_22', 'Sensor_Reading_23', 'Sensor_Reading_24', 'Sensor_Reading_25', 'Sensor_Reading_26', 'Sensor_Reading_27', 'Sensor_Reading_28', 'Sensor_Reading_29', 'Sensor_Reading_30', 'Sensor_Reading_31', 'Sensor_Reading_32', 'Sensor_Reading_33', 'Sensor_Reading_34', 'Sensor_Reading_35', 'Sensor_Reading_36', 'Sensor_Reading_37', 'Sensor_Reading_38', 'Sensor_Reading_39', 'Sensor_Reading_40', 'Sensor_Reading_41', 'Sensor_Reading_42', 'Sensor_Reading_43', 'Sensor_Reading_44', 'Sensor_Reading_45', 'Sensor_Reading_46', 'Sensor_Reading_47', 'Sensor_Reading_48', 'Sensor_Reading_49', 'Sensor_Reading_50', 'Sensor_Reading_51', 'Sensor_Reading_52', 'Sensor_Reading_53', 'Sensor_Reading_54', 'Sensor_Reading_55', 'Sensor_Reading_56', 'Sensor_Reading_57', 'Sensor_Reading_58', 'Sensor_Reading_59', 'Sensor_Reading_60', 'Sensor_Reading_61']\r\n",
                "FROM Truck_Sensor_Readings'\r\n",
                "EXEC [dbo].[PredictBattLife] 'pdm', @query_string;\r\n",
                "\r\n",
                "</pre>\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "04647e35-4f0f-4e52-a89c-6d695809da14"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Next Steps: Continue on to other workloads in SQL Server 2019\r\n",
                "\r\n",
                "Now you're ready to work with SQL Server 2019's other features - [you can learn more about those here](https://docs.microsoft.com/en-us/sql/big-data-cluster/big-data-cluster-overview?view=sqlallproducts-allversions)."
            ],
            "metadata": {
                "azdata_cell_guid": "c7122be0-61a8-4b8d-a023-14212908269d"
            }
        }
    ]
}